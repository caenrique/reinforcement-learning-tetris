# TODO

+ Ajustar el algoritmo de aprendizaje

+ Función de projección del estado completo a un estado simplificado
    1. Estado basado en descriptores
        - Altura media
        - diferencia agregada de alturas maximas
        - número de huecos
    2. Estado basado en simplificación sin tener en cuenta los huecos. Array del tamaño del ancho, donde cada valor
        representa la altura máxima de esa columna

+ Jugador con dos métodos.
    1. Método que juegue usando la técnica greedy. Que reciba Q
    2. Método que juegue entrenando que reciba Q y una instancia del Entrenador

+ Interfaz de consola que tenga las opciones de:
    1. ruta de la información serializada de Q. Usa esto para comenzar a entrenar a partir de ahí
    2. ruta donde guardar el resultado serializado de Q
    3. flag para jugar entrenando o sin entrenar

+ Usar un método de decrementación de alpha que dependa de la cantidad de steps jugadas, para que explore más al principio
    y vaya siguiendo más la policy paulatinamente

# DONE

+ Adaptación de la representación de los mensajes a la última versión

+ Implement random player

+ Filtrar las paredes en el mensaje del estado enviado por el servidor

+ Función de projección del estado completo a un estado simplificado
    3. Estado basado en la condición de si cabe o no la pieza

+ Generador de acciones que reciba el estado actual S y la pieza actual P y devuelva un conjunto de acciones representadas
    como dos números: rotación y desplazamiento

+ Método de valor por defecto a 0

+ Método de selección de acción que reciba un estado S y una función de Seq[(A, V)] -> A donde A es una acción y V es su valor

+ definición de policy que recibe una lista de (A, V) y devuelva una A, donde A es una Acción y V es su valor
    1. e-greedy
    2. on-policy
    3. random

+ Entrenador con un método train() que reciba la tupla (S, R, S') y Q y devuelva Q'

+ Método que unifique el movimiento a realizar según la acción A y un movimiento de bajar completamente la pieza

+ Reimplementar Jugador en Scala para controlar mejor el ciclo de vida del agente

+ Implementar un método de reinicio de la partida que permita jugar partidas en base a una restartPolicy

+ Serializacion de la infomación de la QFunction para poder guardar el resultado del aprendizaje

